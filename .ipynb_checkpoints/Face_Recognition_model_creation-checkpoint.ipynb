{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------------------------------------------------\n",
    "# Face Recognition model creation\n",
    "## ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2                         # for Computer Vission\n",
    "import numpy as np \n",
    "from os import listdir             # to run OS commands\n",
    "from os.path import isfile, join   # To use paths of OS module\n",
    "import os                          # to run OS commands\n",
    "face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')  # Load Model to detect a faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------------------------------------------------\n",
    "# Person1 model training\n",
    "## ------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Person1 Samples\n",
    "## -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.2) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-i3ohmhl0\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-57362a49d9e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[0mperson1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# call function to take sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m \u001b[0mperson1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[0mperson1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[0mperson1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-57362a49d9e9>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Capture image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mface_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Call function if image present thn run below lines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[0mface\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mface_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# resize window\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-57362a49d9e9>\u001b[0m in \u001b[0;36mface_extractor\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mface_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Converts from 3d to 2d i.e., color to gray(white n black)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mfaces\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaleFactor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminNeighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Detects face\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#   scaleFactor      Parameter specifying how much the image size is reduced at each image scale.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.2) C:\\Users\\runneradmin\\AppData\\Local\\Temp\\pip-req-build-i3ohmhl0\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "# Clean Code\n",
    "class create_train:\n",
    "    def __init__(self, person=\"1\"):\n",
    "        person_model = \"\"\n",
    "        self.person = person\n",
    "        self.person_model = person_model\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        count = 0\n",
    "    \n",
    "        self.displayTrain = lambda: print(f\"{'-'*40}\\n\\nPerson{self.person} Model trained sucessefully.\\n\\n{'-'*40}\\n\")\n",
    "        self.displaySample = lambda: print(f\"{'-'*40}\\n\\nCollecting Person{self.person} Samples Completed.\\n\\n{'-'*40}\\n\")\n",
    "        self.displaySave = lambda: print(f\"{'-'*40}\\n\\nPerson{self.person} trained model is saved in path *./saved_model/person{self.person}_model.yml *\\n\\n{'-'*40}\\n\")\n",
    "                                \n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    def face_extractor(self,img):   \n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)  # Converts from 3d to 2d i.e., color to gray(white n black) \n",
    "        faces = face_classifier.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)   \n",
    "        # Detects face\n",
    "    #   scaleFactor      Parameter specifying how much the image size is reduced at each image scale.\n",
    "    #   minNeighbors: higher value results in less detections but with higher quality. \n",
    "\n",
    "        if faces is (): # returns none when no face found\n",
    "            return None\n",
    "        # Crop all faces found\n",
    "        for (x,y,w,h) in faces: # x= Xaxis,y=Yaxis, w=width,h=height\n",
    "            cropped_face = img[y:y+h, x:x+w] # Croping image start from Y axis till height(h), start from X axis till width(w)\n",
    "        return cropped_face \n",
    "                            \n",
    "    # Collect 100 samples of your face from webcam input\n",
    "    def sample(self):\n",
    "        cap = cv2.VideoCapture(0)    # Initialize Webcam\n",
    "        count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()  # Capture image\n",
    "            cv2.imshow(\"hi\",frame)\n",
    "            if self.face_extractor(frame) is not None: # Call function if image present thn run below lines\n",
    "                count += 1  \n",
    "                face = cv2.resize(self.face_extractor(frame), (200, 200))    # resize window\n",
    "                face = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)           # Convert to gray image\n",
    "\n",
    "                # Save file in specified directory with unique name\n",
    "                file_name_path = f'./faces/person{self.person}/' + str(count) + '.jpg'\n",
    "                cv2.imwrite(file_name_path, face)\n",
    "\n",
    "                # Put count on images and display live count\n",
    "                cv2.putText(face, str(count), (50, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "                cv2.imshow('Face Cropper', face)\n",
    "            else:\n",
    "                print(\"Face not found\")\n",
    "                pass\n",
    "            if cv2.waitKey(1) == 13 or count == 200: #13 is the Enter Key\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()    # Stop windows\n",
    "        self.displaySample()\n",
    "        \n",
    "    def train_model(self):\n",
    "        data_path = f'./faces/person{ self.person }/'          # Get the training data we previously made\n",
    "        onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path, f))] # Collect Image names \n",
    "        onlyfiles.remove(\".gitkeep\") \n",
    "        Training_Data, Labels = [], []         # Create arrays for training data and labels\n",
    "        for i, files in enumerate(onlyfiles):         # Open training images in our datapath # Create a numpy array for training data\n",
    "            image_path = data_path + onlyfiles[i] # COmbine path and Image name\n",
    "            images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            Training_Data.append(np.asarray(images, dtype=np.uint8))  # Store image to data in array --> asarray\n",
    "            Labels.append(i)  # Append labels 1-200\n",
    "        # Create a numpy array for both training data and labels\n",
    "        Labels = np.asarray(Labels, dtype=np.int32)\n",
    "\n",
    "        # Initialize facial recognizer\n",
    "        # model = cv2.face.createLBPHFaceRecognizer()\n",
    "        # NOTE: For OpenCV 3.0 use cv2.face.createLBPHFaceRecognizer()\n",
    "        # pip install opencv-contrib-python\n",
    "        # model = cv2.createLBPHFaceRecognizer()\n",
    "\n",
    "        self.person_model  = cv2.face_LBPHFaceRecognizer.create() # Create A model for images\n",
    "        self.person_model.train(np.asarray(Training_Data), np.asarray(Labels))        # Let's train our model \n",
    "        self.displayTrain()\n",
    "    def save_model(self): # save model to file\n",
    "        self.person_model.save(f'./saved_model/person{self.person}_model.yml')\n",
    "        self.displaySave()    \n",
    "         \n",
    "person1 = create_train(\"1\") # call function to take sample\n",
    "person1.sample()\n",
    "person1.train_model()\n",
    "person1.save_model()\n",
    "person2 = create_train(\"2\")\n",
    "person2.sample()\n",
    "person2.train_model()\n",
    "person2.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read() \n",
    "cv2.imshow('Face Cropper', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
